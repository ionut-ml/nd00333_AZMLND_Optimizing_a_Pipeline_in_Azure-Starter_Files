# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This project uses the UCI Bank Marketing dataset, which contains demographic (i.e.: age, education, etc.) and financial (i.e.: loans, housing, consumer price index, etc.) as well as other (e..g: previouse campaing outcome) information about potential customers for bank's marketing campain. Our goal is to predict if the customer answers "yes" or "no" to term deposit. To this end we will use classification algorithms in Azure in 2 ways: using Hyperdrive, and using AutoML.

The best performing model was the AutoML trained VotingEnsemble with 0.915 Accuracy, only slightly above the Logistic Regresion from scikit-learn that we tuned with Hyperdrive and got 0.910 Accuracy.


## Scikit-learn Pipeline
1. Preparing environment and resources:
In the Scikit-learn pipeline we first set-up Workspace and an Experiment, then we spun up a 'Standard D2 V2' compute cluster.

2. Preparing data and estimator
We used our train.py Python script to:
* read in the BankMarketing CSV into a TabularDatasetFactory dataset;
* clean the data (drop unnecessary columns, one-hot-encode categorical columns, etc.);
* split the data into training and test sets;
* Parse arguments as hyperparameters into scikit-learn's Logistic regression classification algorithm.

Next, we used the compute cluster and the train.py script to set up the SKLearn estimator instance.

3. Tune hyperparameters and train classification algorithm
We set up a Random Parameter Sampler to randomly choose values for the 2 paramaters of our scikit-learn classification algorithm (C, the inverse of regularization strength, and max_iter, the maximum iterations taken to converge).

Then, we used BanditPolicy for our early stopping policy used to cancel runs based on a slock factor, or distance from best performing run.

Next, we used the estimator, early stopping policy and random parameter sampler, and others as our configuration for Hyperdrive to tune the hyperparameters of the classification algorithm.

Finally, we used hyperdrive as input into the Experiment in order to train our model, find the best run based on 'Accuracy' as our primary metric and save the model using joblib, and register it to our workspace.

See below diagram of the Scikit Learn Pipeline:
![Alt text](./HyperdriveDiagram.jpg)

**What are the benefits of the parameter sampler you chose?**
The main benefit of Random Parameter Sampling is efficient in both time and resources by picking random values to use for hyperparameters and it yields almost the same performance as Grid Sampling, which is an exhaustive search of all possible value combinations. It is also better for savings compared to the Bayesian Sampling, which picks samples based on previous runs, thus it needs to be bugdeted accordingly.

**What are the benefits of the early stopping policy you chose?**
The BanditPolicy early stopping cancels runs if they exceed a set distance from the best run. The main benefit of this policy it can be used for more aggressive savings with a small slack (like 0.1 used by us). It only keeps the most promising runs and saves costs, run times, and resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
